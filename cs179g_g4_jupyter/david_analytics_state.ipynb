{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15605f07-6e6d-4653-b6eb-ca3cc5ab05c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 01:33:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/01 01:33:07 WARN TaskSetManager: Stage 0 contains a task of very large size (3964 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 01:33:09 WARN TaskSetManager: Stage 1 contains a task of very large size (3964 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:09 WARN TaskSetManager: Stage 2 contains a task of very large size (3964 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:10 WARN TaskSetManager: Stage 3 contains a task of very large size (3964 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:11 WARN TaskSetManager: Stage 4 contains a task of very large size (3964 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:11 WARN TaskSetManager: Stage 5 contains a task of very large size (3964 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:12 WARN TaskSetManager: Stage 6 contains a task of very large size (3964 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:12 WARN TaskSetManager: Stage 7 contains a task of very large size (3964 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:13 WARN TaskSetManager: Stage 8 contains a task of very large size (3964 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:14 WARN TaskSetManager: Stage 9 contains a task of very large size (3964 KiB). The maximum recommended task size is 1000 KiB.\n",
      "1 executors, time= 7.798341274261475\n",
      "22/12/01 01:33:15 WARN TaskSetManager: Stage 0 contains a task of very large size (1985 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 01:33:16 WARN TaskSetManager: Stage 1 contains a task of very large size (1985 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:17 WARN TaskSetManager: Stage 2 contains a task of very large size (1985 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:17 WARN TaskSetManager: Stage 3 contains a task of very large size (1985 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:18 WARN TaskSetManager: Stage 4 contains a task of very large size (1985 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:18 WARN TaskSetManager: Stage 5 contains a task of very large size (1985 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:19 WARN TaskSetManager: Stage 6 contains a task of very large size (1985 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:19 WARN TaskSetManager: Stage 7 contains a task of very large size (1985 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:20 WARN TaskSetManager: Stage 8 contains a task of very large size (1985 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:20 WARN TaskSetManager: Stage 9 contains a task of very large size (1985 KiB). The maximum recommended task size is 1000 KiB.\n",
      "2 executors, time= 5.757989168167114\n",
      "22/12/01 01:33:22 WARN TaskSetManager: Stage 0 contains a task of very large size (1324 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 01:33:23 WARN TaskSetManager: Stage 1 contains a task of very large size (1324 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 01:33:24 WARN TaskSetManager: Stage 2 contains a task of very large size (1324 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:24 WARN TaskSetManager: Stage 3 contains a task of very large size (1324 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:25 WARN TaskSetManager: Stage 4 contains a task of very large size (1324 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:25 WARN TaskSetManager: Stage 5 contains a task of very large size (1324 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:26 WARN TaskSetManager: Stage 6 contains a task of very large size (1324 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 01:33:27 WARN TaskSetManager: Stage 7 contains a task of very large size (1324 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 01:33:28 WARN TaskSetManager: Stage 8 contains a task of very large size (1324 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/12/01 01:33:28 WARN TaskSetManager: Stage 9 contains a task of very large size (1324 KiB). The maximum recommended task size is 1000 KiB.\n",
      "3 executors, time= 6.978494882583618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 executors, time= 6.872924327850342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 executors, time= 7.606982231140137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 executors, time= 7.261057138442993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 executors, time= 8.90510606765747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 executors, time= 8.649485349655151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 executors, time= 8.700912952423096\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from geopy.geocoders import GoogleV3\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, collect_list, concat, lit, regexp_replace, round, split, struct, udf, UserDefinedFunction\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, ArrayType, StructType, StructField\n",
    "\n",
    "for j in range(1,10):\n",
    "    sc = SparkContext(master=f'local[{j}]')\n",
    "    t0 = time()\n",
    "    for i in range(10):\n",
    "        sc.parallelize([1,2] * 1000000).reduce(lambda x,y:x+y)\n",
    "    print(f'{j} executors, time= {time() - t0}')\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdff413d-9cef-429d-9e3d-d15e95e16f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CMTE_ID', 'string'), ('ENTITY_TP', 'string'), ('STATE', 'string'), ('TRANSACTION_AMT', 'int')]\n",
      "[('STATE', 'string'), ('TRANSACTION_AMT', 'int'), ('OTHER_ID', 'string')]\n",
      "[('CAND_ID', 'string'), ('CAND_PCC', 'string')]\n",
      "[('CMTE_ID', 'string'), ('CAND_ID', 'string')]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('analytics-tables').getOrCreate()\n",
    "\n",
    "allIndivSchema = StructType([\n",
    "    StructField(\"CMTE_ID\", StringType(), False),\n",
    "    StructField(\"AMNDT_IND\", StringType(), True),\n",
    "    StructField(\"RPT_TP\", StringType(), True),\n",
    "    StructField(\"TRANSACTION_PGI\", StringType(), True),\n",
    "    StructField(\"IMAGE_NUM\", StringType(), True),\n",
    "    StructField(\"TRANSACTION_TP\", StringType(), True),\n",
    "    StructField(\"ENTITY_TP\", StringType(), True),\n",
    "    StructField(\"NAME\", StringType(), True),\n",
    "    StructField(\"CITY\", StringType(), True),\n",
    "    StructField(\"STATE\", StringType(), True),\n",
    "    StructField(\"ZIP_CODE\", StringType(), True),\n",
    "    StructField(\"EMPLOYER\", StringType(), True),\n",
    "    StructField(\"OCCUPATION\", StringType(), True),\n",
    "    StructField(\"TRANSACTION_DT\", IntegerType(), True),\n",
    "    StructField(\"TRANSACTION_AMT\", IntegerType(), True),\n",
    "    StructField(\"OTHER_ID\", StringType(), True),\n",
    "    StructField(\"TRAN_ID\", StringType(), True),\n",
    "    StructField(\"FILE_NUM\", IntegerType(), True),\n",
    "    StructField(\"MEMO_CD\", StringType(), True),\n",
    "    StructField(\"MEMO_TEXT\", StringType(), True),\n",
    "    StructField(\"SUB_ID\", StringType(), True)\n",
    "])\n",
    "\n",
    "allIndiv = spark.read.format(\"csv\").options(delimiter='|',header='true').schema(allIndivSchema).load(\"data/indiv_cont/itcon*.txt\")\n",
    "allIndiv = allIndiv.drop(\"AMNDT_IND\",\"RPT_TP\",\"TRANSACTION_PGI\",\"IMAGE_NUM\",\"TRANSACTION_TP\",\"NAME\",\"CITY\",\"ZIP_CODE\",\"EMPLOYER\",\"OCCUPATION\",\"TRANSACTION_DT\",\"TRANSACTION_TP\",\"OTHER_ID\",\"TRAN_ID\",\"FILE_NUM\",\"MEMO_CD\",\"MEMO_TEXT\",\"SUB_ID\")\n",
    "allIndiv.createOrReplaceTempView(\"allInd\")\n",
    "\n",
    "commToCand = spark.read.format(\"csv\").options(delimiter='|',inferschema='true',header='true').load('data/itpas2.txt')\n",
    "commToCand = commToCand.drop('CMTE_ID',\"AMNDT_IND\",\"RPT_TP\",\"TRANSACTION_PGI\",\"IMAGE_NUM\",\"TRANSACTION_TP\",\"ENTITY_TP\",\"NAME\",\"CITY\",\"ZIP_CODE\",\"EMPLOYER\",\"OCCUPATION\",\"TRANSACTION_DT\",\"TRANSACTION_TP\",\"CAND_ID\",\"TRAN_ID\",\"FILE_NUM\",\"MEMO_CD\",\"MEMO_TEXT\",\"SUB_ID\")\n",
    "commToCand.createOrReplaceTempView(\"allCom\")\n",
    "\n",
    "candMaster = spark.read.format(\"csv\").options(delimiter='|',inferschema='true',header='true').load('data/cn.txt')\n",
    "candMaster = candMaster.drop(\"CAND_NAME\",\"CAND_PTY_AFFILIATION\",\"CAND_ELECTION_YR\",\"CAND_OFFICE_ST\",\"CAND_OFFICE\",\"CAND_OFFICE_DISTRICT\",\"CAND_ICI\",\"CAND_STATUS\",\"CAND_ST1\",\"CAND_ST2\",\"CAND_CITY\",\"CAND_ST\",\"CAND_ZIP\")\n",
    "candMaster.createOrReplaceTempView(\"candMast\")\n",
    "\n",
    "comMaster = spark.read.format(\"csv\").options(delimiter='|',inferschema='true',header='true').load('data/cm.txt')\n",
    "comMaster = comMaster.drop(\"CMTE_NM\",\"TRES_NM\",\"CMTE_ST1\",\"CMTE_ST2\",\"CMTE_CITY\",\"CMTE_ST\",\"CMTE_ZIP\",\"CMTE_DSGN\",\"CMTE_TP\",\"CMTE_PTY_AFFILIATION\",\"CMTE_FILING_FREQ\",\"ORG_TP\",\"CONNECTED_ORG_NM\")\n",
    "comMaster.createOrReplaceTempView(\"comMast\")\n",
    "\n",
    "print(allIndiv.dtypes)\n",
    "print(commToCand.dtypes)\n",
    "print(candMaster.dtypes)\n",
    "print(comMaster.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1486f31e-f05d-4ffa-b670-207dd83818ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y = spark.sql(\"\"\"\n",
    "# SELECT candMast.CAND_ID AS ID,\n",
    "#     (CASE WHEN allInd.STATE IS NULL THEN \"--\" ELSE allInd.STATE END) AS STATE,\n",
    "#     SUM(allInd.TRANSACTION_AMT) AS TOTAL_AMT_IND\n",
    "# FROM allInd, candMast\n",
    "# WHERE candMast.CAND_PCC=allInd.CMTE_ID AND (allInd.ENTITY_TP=\"CAN\" OR allInd.ENTITY_TP=\"IND\")\n",
    "# GROUP BY candMast.CAND_ID, STATE\n",
    "# ORDER BY ID, TOTAL_AMT_IND DESC\n",
    "# \"\"\")\n",
    "\n",
    "# z = spark.sql(\"\"\"\n",
    "# SELECT candMast.CAND_ID AS ID,\n",
    "#     (CASE WHEN allInd.STATE IS NULL THEN \"--\" ELSE allInd.STATE END) AS STATE,\n",
    "#     SUM(allInd.TRANSACTION_AMT) AS TOTAL_AMT_IND\n",
    "# FROM allInd, candMast, comMast\n",
    "# WHERE comMast.CAND_ID=candMast.CAND_ID AND (comMast.CMTE_ID=allInd.CMTE_ID OR comMast.CAND_ID=allInd.CMTE_ID) AND (allInd.ENTITY_TP=\"CAN\" OR allInd.ENTITY_TP=\"IND\")\n",
    "# GROUP BY candMast.CAND_ID, STATE\n",
    "# ORDER BY ID, TOTAL_AMT_IND\n",
    "# \"\"\")\n",
    "\n",
    "# comMast.CAND_ID=candMast.CAND_ID AND (comMast.CMTE_ID=allCom.OTHER_ID OR comMast.CAND_ID=allCom.OTHER_ID)\n",
    "\n",
    "# +---------+-----+-------------+\n",
    "# |       ID|STATE|TOTAL_AMT_IND|\n",
    "# +---------+-----+-------------+\n",
    "# |H0AL01055|   AL|        24825|\n",
    "# |H0AL01055|   VA|         4750|\n",
    "# |H0AL01055|   DC|         4400|\n",
    "# |H0AL01055|   TX|         4400|\n",
    "# |H0AL01055|   CO|         1500|\n",
    "# |H0AL01055|   MD|         1000|\n",
    "# |H0AL01055|   FL|          500|\n",
    "# |H0AL01055|   AZ|           10|\n",
    "# |H0AL01055|   CA|           10|\n",
    "# |H0AL02202|   AL|         7647|\n",
    "# |H0AL02202|   --|          501|\n",
    "# |H0AL05163|   AL|       551346|\n",
    "# |H0AL05163|   FL|        65967|\n",
    "# |H0AL05163|   CA|        55775|\n",
    "# |H0AL05163|   NC|        47915|\n",
    "# |H0AL05163|   TN|        38485|\n",
    "# |H0AL05163|   TX|        27005|\n",
    "# |H0AL05163|   GA|        23291|\n",
    "# |H0AL05163|   WI|        12900|\n",
    "# |H0AL05163|   PA|        11497|\n",
    "# +---------+-----+-------------+\n",
    "# only showing top 20 rows\n",
    "\n",
    "# +---------+-----+-------------+\n",
    "# |       ID|STATE|TOTAL_AMT_IND|\n",
    "# +---------+-----+-------------+\n",
    "# |H0AL01055|   AZ|           10|\n",
    "# |H0AL01055|   CA|           10|\n",
    "# |H0AL01055|   FL|          500|\n",
    "# |H0AL01055|   MD|         1000|\n",
    "# |H0AL01055|   CO|         1500|\n",
    "# |H0AL01055|   TX|         4400|\n",
    "# |H0AL01055|   DC|         4400|\n",
    "# |H0AL01055|   VA|         4750|\n",
    "# |H0AL01055|   AL|        24825|\n",
    "# |H0AL02202|   --|          501|\n",
    "# |H0AL02202|   AL|         7647|\n",
    "# |H0AL07086|   PA|           25|\n",
    "# |H0AL07086|   GA|          150|\n",
    "# |H0AL07086|   TX|          250|\n",
    "# |H0AL07086|   NJ|          250|\n",
    "# |H0AL07086|   LA|          320|\n",
    "# |H0AL07086|   AZ|          500|\n",
    "# |H0AL07086|   SC|          505|\n",
    "# |H0AL07086|   VI|          600|\n",
    "# |H0AL07086|   PR|         1000|\n",
    "# +---------+-----+-------------+\n",
    "# only showing top 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "039f84fc-a52f-4c9f-ac1f-56ef0d0b8e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from geopy.geocoders       import GoogleV3\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types     import FloatType, ArrayType\n",
    "\n",
    "# geolocator = GoogleV3(api_key=\"AIzaSyC0GaU8jaB9fesUuG7sAQz07g-R5ZRBP7Q\")\n",
    "\n",
    "# stateTableAllCOM = spark.sql(\"\"\"\n",
    "# SELECT Y.CAND_ID,\n",
    "#     (CASE WHEN X.STATE IS NULL THEN \"--\" ELSE X.STATE END) AS STATE,\n",
    "#     SUM(X.TRANSACTION_AMT) AS TOTAL_AMT_IND\n",
    "# FROM allCom AS X, candMast AS Y\n",
    "# WHERE Y.CAND_PCC=X.OTHER_ID\n",
    "# GROUP BY Y.CAND_ID, STATE\n",
    "# ORDER BY Y.CAND_ID, TOTAL_AMT_IND DESC\n",
    "# \"\"\")\n",
    "\n",
    "# +---------+-----+-------------+\n",
    "# |  CAND_ID|STATE|TOTAL_AMT_IND|\n",
    "# +---------+-----+-------------+\n",
    "# |H0AL01055|   AL|       415955|\n",
    "# |H0AL01055|   VA|         1000|\n",
    "# |H0AL02087|   AL|        -1000|\n",
    "# |H0AL02202|   AL|         5000|\n",
    "# |H0AL05163|   AL|       115607|\n",
    "# |H0AL05163|   VA|         5000|\n",
    "# |H0AL07086|   AL|      1286900|\n",
    "# |H0AL07086|   DC|       166395|\n",
    "# |H0AL07086|   NJ|         5000|\n",
    "# |H0AL07086|   MD|         1156|\n",
    "# |H0AL07086|   VA|          400|\n",
    "# |H0AR01083|   AR|       436400|\n",
    "# |H0AR01083|   DC|        33500|\n",
    "# |H0AR01083|   VA|        24000|\n",
    "# |H0AR01083|   TX|         2500|\n",
    "# |H0AR01083|   GA|         2000|\n",
    "# |H0AR03055|   AR|       643400|\n",
    "# |H0AR03055|   DC|        66000|\n",
    "# |H0AR03055|   VA|         6000|\n",
    "# |H0AR03055|   AL|         3000|\n",
    "# +---------+-----+-------------+\n",
    "# only showing top 20 rows\n",
    "\n",
    "# location = geolocator.geocode(\"Arkansas\")\n",
    "# print(location)\n",
    "# result = str((location.latitude, location.longitude))\n",
    "# print(result)\n",
    "\n",
    "\n",
    "# location = geolocator.geocode(\"Arizona\")\n",
    "# print(location)\n",
    "# result = str((location.latitude, location.longitude))\n",
    "# print(result)\n",
    "\n",
    "\n",
    "# location = geolocator.geocode(\"Illinois\")\n",
    "# print(location)\n",
    "# result = str((location.latitude, location.longitude))\n",
    "# print(result)\n",
    "\n",
    "\n",
    "# location = geolocator.geocode(\"Wyoming\")\n",
    "# print(location)\n",
    "# result = str((location.latitude, location.longitude))\n",
    "# print(result)\n",
    "\n",
    "# dick = { 'AL':'Alabama', 'AK':'Alaska', 'AR':'Arizona', 'AR':'Arkansas', 'AS':'American Samoa',\n",
    "#         'CA':'California', 'CO':'Colorado', 'CT':'Connecticut', 'DE':'Delaware', 'DC':'District of Columbia',\n",
    "#         'FL':'Florida', 'GA':'Georgia', 'GU':'Guam', 'HI':'Hawaii', 'ID':'Idaho',\n",
    "#         'IL':'Illinois', 'IN':'Indiana', 'IA':'Iowa', 'KS':'Kansas', 'KY':'Kentucky',\n",
    "#         'LA':'Louisiana', 'ME':'Maine', 'MD':'Maryland', 'MA':'Massachusetts', 'MI':'Michigan',\n",
    "#         'MN':'Minnesota', 'MS':'Mississippi', 'MO':'Missouri', 'MT':'Montana', 'NE':'Nebraska',\n",
    "#         'NV':'Nevada', 'NH':'New Hampshire', 'NJ':'New Jersey', 'NM':'New Mexico', 'NY':'New York',\n",
    "#         'NC':'North Carolina', 'ND':'North Dakota', 'MP':'Northern Mariana Islands', 'OH':'Ohio', 'OK':'Oklahoma',\n",
    "#         'OR':'Oregon', 'PA':'Pennsylvania', 'PR':'Puerto Rico', 'RI':'Rhode Island', 'SC':'South Carolina',\n",
    "#         'SD':'South Dakota', 'TN':'Tennessee', 'TX':'Texas', 'UT':'Utah', 'VT':'Vermont',\n",
    "#         'VA':'Virginia', 'VI':'Virgin Islands', 'WA':'Washington', 'WV':'West Virginia', 'WI':'Wisconsin',\n",
    "#         'WT':'Wyoming' }\n",
    "\n",
    "# for i in dick:\n",
    "#     location = geolocator.geocode(dick[i])\n",
    "#     dick[i] = str((location.latitude, location.longitude))\n",
    "\n",
    "# def convert_state(s):\n",
    "#     return dick.get(s, s)\n",
    "\n",
    "# conv_state = UserDefinedFunction(lambda s : convert_state(s), StringType())\n",
    "# spark.udf.register(\"convert_state\", conv_state)\n",
    "\n",
    "# stateTableAllCOM = stateTableAllCOM.withColumn('COORD', conv_state('STATE')).show()\n",
    "\n",
    "# stateTableAllCOM = spark.sql(\"\"\"\n",
    "# SELECT Y.CAND_ID,\n",
    "#     (CASE WHEN X.STATE IS NULL THEN \"--\" ELSE X.STATE END) AS STATE,\n",
    "#     SUM(X.TRANSACTION_AMT) AS TOTAL_AMT_IND,\n",
    "#     (CASE WHEN X.STATE IS NULL THEN \"--\" ELSE convert_state(X.STATE) END) AS COORD\n",
    "# FROM allCom AS X, candMast AS Y\n",
    "# WHERE Y.CAND_PCC=X.OTHER_ID\n",
    "# GROUP BY Y.CAND_ID, STATE\n",
    "# ORDER BY Y.CAND_ID, TOTAL_AMT_IND DESC\n",
    "# \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e997887b-9c74-486f-8e2c-79be50fddf5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rank by total individual contributions by state for ONE valid candidate\n",
    "# =========\n",
    "# stateTableIND = spark.sql(\"\"\"\n",
    "# SELECT Y.CAND_ID, Y.CAND_NAME,\n",
    "#     (CASE WHEN X.STATE IS NULL THEN \"--\" ELSE X.STATE END) AS STATE,\n",
    "#     SUM(X.TRANSACTION_AMT) AS TOTAL_AMT_IND\n",
    "# FROM allInd AS X, candMast AS Y\n",
    "# WHERE Y.CAND_ID=\"H0IA01174\" AND Y.CAND_PCC=X.CMTE_ID AND (X.ENTITY_TP=\"CAN\" OR X.ENTITY_TP=\"IND\")\n",
    "# GROUP BY Y.CAND_ID, Y.CAND_NAME, STATE\n",
    "# ORDER BY Y.CAND_ID, Y.CAND_NAME, TOTAL_AMT_IND DESC\n",
    "# \"\"\").show()\n",
    "\n",
    "# Rank by total committee contributions by state for ONE valid candidate\n",
    "# =========\n",
    "# stateTableCOM = spark.sql(\"\"\"\n",
    "# SELECT Y.CAND_ID, Y.CAND_NAME,\n",
    "#     (CASE WHEN X.STATE IS NULL THEN \"--\" ELSE X.STATE END) AS STATE,\n",
    "#     SUM(X.TRANSACTION_AMT) AS TOTAL_AMT_IND\n",
    "# FROM allCom AS X, candMast AS Y\n",
    "# WHERE Y.CAND_ID=\"H0IA01174\" AND Y.CAND_PCC=X.OTHER_ID\n",
    "# GROUP BY Y.CAND_ID, Y.CAND_NAME, STATE\n",
    "# ORDER BY Y.CAND_ID, Y.CAND_NAME, TOTAL_AMT_IND DESC\n",
    "# \"\"\").show()\n",
    "\n",
    "# Rank by total individual/committee/individual+committee contributions by state for ONE valid candidate\n",
    "# =========\n",
    "# stateTableALL = spark.sql(\"\"\"\n",
    "# SELECT stateIND.CAND_ID AS ID, stateIND.CAND_NAME AS NAME, stateIND.STATE,\n",
    "#     (CASE WHEN stateIND.TOTAL_AMT_IND IS NULL THEN 0 ELSE stateIND.TOTAL_AMT_IND END)+\n",
    "#     (CASE WHEN stateCOM.TOTAL_AMT_COM IS NULL THEN 0 ELSE stateCOM.TOTAL_AMT_COM END) AS TOTAL,\n",
    "#     (CASE WHEN stateIND.TOTAL_AMT_IND IS NULL THEN 0 ELSE stateIND.TOTAL_AMT_IND END) AS INDIVIDUAL,\n",
    "#     (CASE WHEN stateCOM.TOTAL_AMT_COM IS NULL THEN 0 ELSE stateCOM.TOTAL_AMT_COM END) AS COMMITTEE\n",
    "# FROM (\n",
    "#     SELECT Y.CAND_ID, Y.CAND_NAME,\n",
    "#         (CASE WHEN X.STATE IS NULL THEN \"--\" ELSE X.STATE END) AS STATE,\n",
    "#         SUM(X.TRANSACTION_AMT) AS TOTAL_AMT_IND\n",
    "#     FROM allInd AS X, candMast AS Y\n",
    "#     WHERE Y.CAND_ID=\"H0IA01174\" AND Y.CAND_PCC=X.CMTE_ID AND (X.ENTITY_TP=\"CAN\" OR X.ENTITY_TP=\"IND\")\n",
    "#     GROUP BY Y.CAND_ID, Y.CAND_NAME, X.STATE\n",
    "#     ) AS stateIND\n",
    "# FULL JOIN (\n",
    "#     SELECT Y.CAND_ID, Y.CAND_NAME,\n",
    "#         (CASE WHEN X.STATE IS NULL THEN \"--\" ELSE X.STATE END) AS STATE,\n",
    "#         SUM(X.TRANSACTION_AMT) AS TOTAL_AMT_COM\n",
    "#     FROM allCom AS X, candMast AS Y\n",
    "#     WHERE Y.CAND_ID=\"H0IA01174\" AND Y.CAND_PCC=X.OTHER_ID\n",
    "#     GROUP BY Y.CAND_ID, Y.CAND_NAME, STATE\n",
    "#     ) AS stateCOM\n",
    "# WHERE stateIND.STATE=stateCOM.STATE\n",
    "# ORDER BY ID, TOTAL DESC\n",
    "# \"\"\").show()\n",
    "\n",
    "# Rank by total individual contributions by state for ALL valid candidates\n",
    "# =========\n",
    "# stateTableAllIND = spark.sql(\"\"\"\n",
    "# SELECT Y.CAND_ID, Y.CAND_NAME,\n",
    "#     (CASE WHEN X.STATE IS NULL THEN \"--\" ELSE X.STATE END) AS STATE,\n",
    "#     SUM(X.TRANSACTION_AMT) AS TOTAL_AMT_IND\n",
    "# FROM allInd AS X, candMast AS Y\n",
    "# WHERE Y.CAND_PCC=X.CMTE_ID AND (X.ENTITY_TP=\"CAN\" OR X.ENTITY_TP=\"IND\") AND Y.CAND_ID IS NOT NULL\n",
    "# GROUP BY Y.CAND_ID, Y.CAND_NAME, STATE\n",
    "# ORDER BY Y.CAND_ID, Y.CAND_NAME, TOTAL_AMT_IND DESC\n",
    "# \"\"\").show()\n",
    "\n",
    "# Rank by total committee contributions by state for ALL valid candidates\n",
    "# =========\n",
    "# stateTableAllCOM = spark.sql(\"\"\"\n",
    "# SELECT Y.CAND_ID, Y.CAND_NAME,\n",
    "#     (CASE WHEN X.STATE IS NULL THEN \"--\" ELSE X.STATE END) AS STATE,\n",
    "#     SUM(X.TRANSACTION_AMT) AS TOTAL_AMT_IND\n",
    "# FROM allCom AS X, candMast AS Y\n",
    "# WHERE Y.CAND_PCC=X.OTHER_ID\n",
    "# GROUP BY Y.CAND_ID, Y.CAND_NAME, STATE\n",
    "# ORDER BY Y.CAND_ID, Y.CAND_NAME, TOTAL_AMT_IND DESC\n",
    "# \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e64854-e18a-404d-8bf7-b70fd0042da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rank by total individual/committee/individual+committee contributions by state for ONE valid candidate (same as next chunk but split)\n",
    "# ========\n",
    "\n",
    "# stateTableAllIND = spark.sql(\"\"\"\n",
    "# SELECT Y.CAND_ID, Y.CAND_NAME,\n",
    "#     (CASE WHEN X.STATE IS NULL THEN \"--\" ELSE X.STATE END) AS STATE,\n",
    "#     SUM(X.TRANSACTION_AMT) AS TOTAL_AMT_IND\n",
    "# FROM allInd AS X, candMast AS Y\n",
    "# WHERE Y.CAND_PCC=X.CMTE_ID AND (X.ENTITY_TP=\"CAN\" OR X.ENTITY_TP=\"IND\")\n",
    "# GROUP BY Y.CAND_ID, Y.CAND_NAME, STATE\n",
    "# \"\"\")\n",
    "\n",
    "# stateTableAllCOM = spark.sql(\"\"\"\n",
    "# SELECT Y.CAND_ID, Y.CAND_NAME,\n",
    "#     (CASE WHEN X.STATE IS NULL THEN \"--\" ELSE X.STATE END) AS STATE,\n",
    "#     SUM(X.TRANSACTION_AMT) AS TOTAL_AMT_COM\n",
    "# FROM allCom AS X, candMast AS Y\n",
    "# WHERE Y.CAND_PCC=X.OTHER_ID\n",
    "# GROUP BY Y.CAND_ID, Y.CAND_NAME, STATE\n",
    "# \"\"\")\n",
    "\n",
    "# stateTableAllIND.createOrReplaceTempView(\"stateIND\")\n",
    "# stateTableAllCOM.createOrReplaceTempView(\"stateCOM\")\n",
    "\n",
    "# stateALL = spark.sql(\"\"\"\n",
    "# SELECT stateIND.CAND_ID AS ID, stateIND.CAND_NAME AS NAME, stateIND.STATE,\n",
    "#     (CASE WHEN stateIND.TOTAL_AMT_IND IS NULL THEN 0 ELSE stateIND.TOTAL_AMT_IND END)+\n",
    "#     (CASE WHEN stateCOM.TOTAL_AMT_COM IS NULL THEN 0 ELSE stateCOM.TOTAL_AMT_COM END) AS TOTAL,\n",
    "#     (CASE WHEN stateIND.TOTAL_AMT_IND IS NULL THEN 0 ELSE stateIND.TOTAL_AMT_IND END) AS INDIVIDUAL,\n",
    "#     (CASE WHEN stateCOM.TOTAL_AMT_COM IS NULL THEN 0 ELSE stateCOM.TOTAL_AMT_COM END) AS COMMITTEE\n",
    "# FROM stateIND, stateCOM\n",
    "# WHERE stateIND.CAND_ID=stateCOM.CAND_ID AND stateIND.STATE=stateCOM.STATE\n",
    "# ORDER BY ID, TOTAL DESC\n",
    "# \"\"\")\n",
    "\n",
    "# stateALL.show()\n",
    "# type(stateALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7d1c7af-03f8-4ec9-b28f-ce8e5abb75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root\n",
    "#  |-- ID: string (nullable = true)\n",
    "#  |-- donateTable: array (nullable = true)\n",
    "#  |    |-- element: struct (containsNull = false)\n",
    "#  |    |    |-- INDIVIDUAL: string (nullable = true)\n",
    "#  |    |    |-- CITY: string (nullable = true)\n",
    "#  |    |    |-- STATE: string (nullable = true)\n",
    "#  |    |    |-- ZIP_CODE: string (nullable = true)\n",
    "#  |    |    |-- OCCUPATION: string (nullable = true)\n",
    "#  |    |    |-- TOTAL_MONEY_DONATED: long (nullable = true)\n",
    "\n",
    "# Dick = {}\n",
    "# for x in data:\n",
    "#     for donateTableElement in x[\"donateTable\"]:\n",
    "#         zipcode = donateTableElement[\"ZIP_CODE\"]\n",
    "#         if zipcode not in Dick:\n",
    "#             if zipcode == \"\":\n",
    "#                 Dick[zipcode] = \"-\"\n",
    "#             else:\n",
    "#                 location = geolocator.geocode(zipcode)\n",
    "#                 Dick[zipcode] = str((location.latitude, location.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f9a50e3-13d7-4b16-b6df-aba82b24b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- COORD: string (nullable = true)\n",
      " |-- TOTAL: long (nullable = true)\n",
      " |-- INDIVIDUAL: long (nullable = true)\n",
      " |-- COMMITTEE: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geolocator = GoogleV3(api_key=\"AIzaSyC0GaU8jaB9fesUuG7sAQz07g-R5ZRBP7Q\")\n",
    "\n",
    "dick = { 'AL':'Alabama', 'AK':'Alaska', 'AR':'Arizona', 'AR':'Arkansas', 'AS':'American Samoa',\n",
    "        'CA':'California', 'CO':'Colorado', 'CT':'Connecticut', 'DE':'Delaware', 'DC':'District of Columbia',\n",
    "        'FL':'Florida', 'GA':'Georgia', 'GU':'Guam', 'HI':'Hawaii', 'ID':'Idaho',\n",
    "        'IL':'Illinois', 'IN':'Indiana', 'IA':'Iowa', 'KS':'Kansas', 'KY':'Kentucky',\n",
    "        'LA':'Louisiana', 'ME':'Maine', 'MD':'Maryland', 'MA':'Massachusetts', 'MI':'Michigan',\n",
    "        'MN':'Minnesota', 'MS':'Mississippi', 'MO':'Missouri', 'MT':'Montana', 'NE':'Nebraska',\n",
    "        'NV':'Nevada', 'NH':'New Hampshire', 'NJ':'New Jersey', 'NM':'New Mexico', 'NY':'New York',\n",
    "        'NC':'North Carolina', 'ND':'North Dakota', 'MP':'Northern Mariana Islands', 'OH':'Ohio', 'OK':'Oklahoma',\n",
    "        'OR':'Oregon', 'PA':'Pennsylvania', 'PR':'Puerto Rico', 'RI':'Rhode Island', 'SC':'South Carolina',\n",
    "        'SD':'South Dakota', 'TN':'Tennessee', 'TX':'Texas', 'UT':'Utah', 'VT':'Vermont',\n",
    "        'VA':'Virginia', 'VI':'Virgin Islands', 'WA':'Washington', 'WV':'West Virginia', 'WI':'Wisconsin',\n",
    "        'WT':'Wyoming' }\n",
    "\n",
    "# Arizona, Arkansas, Illinois, Wyoming\n",
    "\n",
    "for i in dick:\n",
    "    location = geolocator.geocode(dick[i])\n",
    "    dick[i] = str((location.latitude, location.longitude))\n",
    "\n",
    "def convert_state(s):\n",
    "    return dick.get(s, s)\n",
    "\n",
    "conv_state = UserDefinedFunction(lambda s : convert_state(s), StringType())\n",
    "spark.udf.register(\"convert_state\", conv_state)\n",
    "\n",
    "# Rank by total individual/committee/individual+committee contributions by state for ONE valid candidate\n",
    "# ========\n",
    "stateTableALL = spark.sql(\"\"\"\n",
    "SELECT stateIND.CAND_ID AS ID, stateIND.STATE,\n",
    "    convert_state(stateIND.STATE) AS COORD,\n",
    "    (CASE WHEN stateIND.TOTAL_AMT_IND IS NULL THEN 0 ELSE stateIND.TOTAL_AMT_IND END)+\n",
    "    (CASE WHEN stateCOM.TOTAL_AMT_COM IS NULL THEN 0 ELSE stateCOM.TOTAL_AMT_COM END) AS TOTAL,\n",
    "    (CASE WHEN stateIND.TOTAL_AMT_IND IS NULL THEN 0 ELSE stateIND.TOTAL_AMT_IND END) AS INDIVIDUAL,\n",
    "    (CASE WHEN stateCOM.TOTAL_AMT_COM IS NULL THEN 0 ELSE stateCOM.TOTAL_AMT_COM END) AS COMMITTEE\n",
    "FROM (    \n",
    "    SELECT candMast.CAND_ID,\n",
    "        (CASE WHEN allInd.STATE IS NULL THEN \"--\" ELSE allInd.STATE END) AS STATE,\n",
    "        SUM(allInd.TRANSACTION_AMT) AS TOTAL_AMT_IND\n",
    "    FROM allInd, candMast, comMast\n",
    "    WHERE comMast.CAND_ID=candMast.CAND_ID AND (comMast.CMTE_ID=allInd.CMTE_ID OR comMast.CAND_ID=allInd.CMTE_ID) AND (allInd.ENTITY_TP=\"CAN\" OR allInd.ENTITY_TP=\"IND\")\n",
    "    GROUP BY candMast.CAND_ID, STATE\n",
    "    ) AS stateIND\n",
    "FULL JOIN (\n",
    "    SELECT candMast.CAND_ID,\n",
    "        (CASE WHEN allCom.STATE IS NULL THEN \"--\" ELSE allCom.STATE END) AS STATE,\n",
    "        SUM(allCom.TRANSACTION_AMT) AS TOTAL_AMT_COM\n",
    "    FROM allCom, candMast, comMast\n",
    "    WHERE comMast.CAND_ID=candMast.CAND_ID AND (comMast.CMTE_ID=allCom.OTHER_ID OR comMast.CAND_ID=allCom.OTHER_ID)\n",
    "    GROUP BY candMast.CAND_ID, STATE\n",
    "    ) AS stateCOM\n",
    "WHERE stateIND.CAND_ID=stateCOM.CAND_ID AND stateIND.STATE=stateCOM.STATE\n",
    "ORDER BY ID, TOTAL DESC\n",
    "\"\"\")\n",
    "stateTableALL.createOrReplaceTempView(\"stateTablePre\")\n",
    "type(stateTableALL)\n",
    "stateTableALL.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4ec294a-870c-4d03-ba44-a49d6c2e2335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- stateTable: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = false)\n",
      " |    |    |-- STATE: string (nullable = true)\n",
      " |    |    |-- COORD: string (nullable = true)\n",
      " |    |    |-- TOTAL: long (nullable = true)\n",
      " |    |    |-- INDIVIDUAL: long (nullable = true)\n",
      " |    |    |-- COMMITTEE: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Only ID (distinct rows as KEYs for DynamoDB)\n",
    "# ========\n",
    "uniqueCandidateIdsAndNames = spark.sql(\"\"\"\n",
    "SELECT DISTINCT ID\n",
    "FROM stateTablePre\n",
    "\"\"\")\n",
    "\n",
    "# Everything but NAME (ID for equi-join, the rest to populate the nested table)\n",
    "noName = spark.sql(\"\"\"\n",
    "SELECT ID AS ID, STATE, COORD, TOTAL, INDIVIDUAL, COMMITTEE\n",
    "FROM stateTablePre\n",
    "\"\"\")\n",
    "\n",
    "# With an outer equi-join by (ID, NAME), we nest one or more rows of (STATE, TOTAL, INDIVIDUAL, COMMITTEE) columns for each distinct candidate using the struct type\n",
    "finalTable = uniqueCandidateIdsAndNames.join(\n",
    "    noName\n",
    "        .groupBy(\"ID\")\n",
    "        .agg(collect_list(struct(noName.STATE, noName.COORD, noName.TOTAL, \n",
    "                                 noName.INDIVIDUAL, noName.COMMITTEE)).alias(\"stateTable\"))\n",
    "    , \"ID\"\n",
    "    , \"outer\"\n",
    ")\n",
    "\n",
    "finalTable.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60a8f380-4221-4829-a71a-bcc15397acd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# finalTable.count()\n",
    "# >>\n",
    "# 1507\n",
    "\n",
    "# finalTable.toJSON().first()\n",
    "# >>\n",
    "# '{\"ID\":\"H0AL01055\",\"stateTable\":[{\"STATE\":\"AL\",\"TOTAL\":440780,\"INDIVIDUAL\":24825,\"COMMITTEE\":415955},{\"STATE\":\"VA\",\"TOTAL\":5750,\"INDIVIDUAL\":4750,\"COMMITTEE\":1000}]}'\n",
    "\n",
    "# print(finalTable.tail(5))\n",
    "# >>\n",
    "\n",
    "# finalTable.show(10, False)\n",
    "# >> 40 minutes bro\n",
    "# +---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "# |ID       |stateTable                                                                                                                                                                                                                                       |\n",
    "# +---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "# |H0AL01055|[{AL, 440780, 24825, 415955}, {VA, 5750, 4750, 1000}]                                                                                                                                                                                            |\n",
    "# |H0AL02202|[{AL, 12647, 7647, 5000}]                                                                                                                                                                                                                        |\n",
    "# |H0AL07086|[{AL, 1352020, 65120, 1286900}, {DC, 196905, 30510, 166395}, {CA, 56687, 49341, 7346}, {AZ, 28886, 500, 28386}, {MD, 12607, 11451, 1156}, {FL, 6086, 2250, 3836}, {NJ, 5250, 250, 5000}, {VA, 4650, 4250, 400}]                                  |\n",
    "# |H0AR01083|[{AR, 630144, 193744, 436400}, {DC, 36000, 2500, 33500}, {VA, 27500, 3500, 24000}]                                                                                                                                                               |\n",
    "# |H0AR03055|[{AR, 743755, 100355, 643400}, {DC, 79000, 13000, 66000}, {VA, 9500, 3500, 6000}, {AL, 4500, 1500, 3000}]                                                                                                                                        |\n",
    "# |H0AZ01259|[{AZ, 149238, 97041, 52197}, {NV, 31999, 1999, 30000}, {NY, 18301, 17400, 901}, {IL, 17799, 30, 17769}, {WA, 7159, 6400, 759}, {CA, 5522, 4300, 1222}, {FL, 3255, 3005, 250}, {UT, 3250, 3000, 250}, {PA, 2750, 1500, 1250}, {VA, 690, 250, 440}]|\n",
    "# |H0CA03078|[{CA, 864279, 162225, 702054}, {FL, 33820, 150, 33670}, {DC, 30098, 6550, 23548}, {IL, 10112, 10000, 112}, {NY, 4113, 4105, 8}]                                                                                                                  |\n",
    "# |H0CA03128|[{CA, 147258, 132118, 15140}]                                                                                                                                                                                                                    |\n",
    "# |H0CA06170|[{CA, 24534, 24034, 500}]                                                                                                                                                                                                                        |\n",
    "# |H0CA07137|[{CA, 5025, 4950, 75}]                                                                                                                                                                                                                           |\n",
    "# +---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
    "# only showing top 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ce2c8-a96b-4998-bea4-340d03f248a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalTable.coalesce(1).write.format('json').save('out_analytics_state')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
