{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3968e9e-1528-4112-9045-79fe5d919380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pyspark import SparkContext\n",
    "for j in range(1,10):\n",
    "    sc = SparkContext(master=f'local[{j}]')\n",
    "    t0 = time()\n",
    "    for i in range(10):\n",
    "        sc.parallelize([1,2] * 1000000).reduce(lambda x,y:x+y)\n",
    "    print(f'{j} executors, time= {time() - t0}')\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67c2423-a9da-498a-a4db-6bc29fe0363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from geopy.geocoders import GoogleV3\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, collect_list, concat, lit, regexp_replace, round, split, struct, udf, UserDefinedFunction\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, ArrayType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "157a8d6d-9724-40d6-9b65-d90364981d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "from pyspark.sql.functions import collect_list, split, regexp_replace, col, round,concat,lit,avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564ecdd-1db3-491c-bf8a-a33a210d8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName('explore').getOrCreate()\n",
    "explore = spark.read.format(\"csv\").options(delimiter='|',inferschema='true',header='true').load('data/cn.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f8ed3b-22e4-47f1-a87e-b7d11ba891fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unwanted columns\n",
    "explore = explore.drop(col(\"CAND_ST1\"))\n",
    "explore = explore.drop(col(\"CAND_ST2\"))\n",
    "explore = explore.drop(col(\"CAND_ZIP\"))\n",
    "explore = explore.drop(col(\"CAND_CITY\"))\n",
    "explore = explore.drop(col(\"CAND_ST\"))\n",
    "explore = explore.drop(col(\"CAND_STATUS\"))\n",
    "\n",
    "explore.createOrReplaceTempView(\"explore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eddcfb01-6c4f-4658-a8a0-29eae0339427",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore = spark.sql(\"\"\" SELECT CAND_ID, CAND_NAME, (CASE WHEN e.CAND_PTY_AFFILIATION IS NULL THEN \"-\" ELSE e.CAND_PTY_AFFILIATION END) \n",
    "AS CAND_PTY_AFFILIATION, (CASE WHEN e.CAND_ELECTION_YR IS NULL THEN \"-\" ELSE e.CAND_ELECTION_YR END) AS CAND_ELECTION_YR, \n",
    "(CASE WHEN e.CAND_OFFICE_ST IS NULL THEN \"-\" ELSE e.CAND_OFFICE_ST END) AS CAND_OFFICE_ST, \n",
    "(CASE WHEN e.CAND_OFFICE IS NULL THEN \"-\" ELSE e.CAND_OFFICE END) AS CAND_OFFICE, \n",
    "(CASE WHEN e.CAND_ICI IS NULL THEN \"-\" ELSE e.CAND_ICI END) AS CAND_ICI, \n",
    "(CASE WHEN e.CAND_PCC IS NULL THEN \"-\" ELSE e.CAND_PCC END) AS CAND_PCC, \n",
    "(CASE WHEN e.CAND_OFFICE_DISTRICT IS NULL THEN \"-\" WHEN e.CAND_OFFICE_DISTRICT = \"0\" THEN \"-\" \n",
    "ELSE e.CAND_OFFICE_DISTRICT END) AS CAND_OFFICE_DISTRICT\n",
    "from explore e\"\"\")\n",
    "\n",
    "explore.createOrReplaceTempView(\"explore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3bfc2-f45f-43bc-8efd-fad310496503",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore = spark.sql(\"\"\"\n",
    "SELECT (CAND_OFFICE || CAND_OFFICE_ST || CAND_OFFICE_DISTRICT) AS CID, CAND_ID, CAND_NAME,CAND_PTY_AFFILIATION, CAND_ICI, CAND_ELECTION_YR, CAND_OFFICE_ST, CAND_OFFICE, CAND_OFFICE_DISTRICT, CAND_PCC\n",
    "FROM explore\n",
    "GROUP BY CID, CAND_ID, CAND_NAME, CAND_PTY_AFFILIATION, CAND_ICI, CAND_ELECTION_YR, CAND_OFFICE_ST, CAND_OFFICE, CAND_OFFICE_DISTRICT, CAND_PCC\n",
    "\"\"\")\n",
    "\n",
    "explore.createOrReplaceTempView(\"explore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efad9c-3356-4872-80a0-9eebdcad44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueCID = spark.sql(\"\"\"\n",
    "SELECT DISTINCT CID\n",
    "FROM explore\n",
    "\"\"\")\n",
    "\n",
    "ftable = spark.sql(\"\"\"\n",
    "SELECT CID, CAND_ID, CAND_NAME, CAND_ICI, CAND_PTY_AFFILIATION, CAND_OFFICE, CAND_OFFICE_ST, CAND_OFFICE_DISTRICT, CAND_ELECTION_YR, CAND_PCC\n",
    "FROM explore\n",
    "\"\"\")\n",
    "\n",
    "# With an outer equi-join by (ID, NAME), we nest one or more rows of columns for each distinct candidate using the struct type\n",
    "filtertbl = uniqueCID.join(\n",
    "    ftable\n",
    "        .groupBy(\"CID\")\n",
    "        .agg(collect_list(struct(ftable.CAND_ID, ftable.CAND_NAME, ftable.CAND_ICI, ftable.CAND_OFFICE, ftable.CAND_OFFICE_ST, ftable.CAND_OFFICE_DISTRICT, ftable.CAND_ELECTION_YR, ftable.CAND_PCC)).alias(\"candidate\"))\n",
    "    , \"CID\"\n",
    "    , \"outer\"\n",
    ")\n",
    "\n",
    "filtertbl.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02766379-27cf-4f98-a741-5f4d5d5b8717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders       import GoogleV3\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types     import FloatType, ArrayType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f95a82-13f8-41b5-9717-84abdf9fa9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = GoogleV3(api_key=\"AIzaSyC0GaU8jaB9fesUuG7sAQz07g-R5ZRBP7Q\")\n",
    "\n",
    "dick = { 'AL':'Alabama', 'AK':'Alaska', 'AR':'Arizona', 'AR':'Arkansas', 'AS':'American Samoa',\n",
    "        'CA':'California', 'CO':'Colorado', 'CT':'Connecticut', 'DE':'Delaware', 'DC':'District of Columbia',\n",
    "        'FL':'Florida', 'GA':'Georgia', 'GU':'Guam', 'HI':'Hawaii', 'ID':'Idaho',\n",
    "        'IL':'Illinois', 'IN':'Indiana', 'IA':'Iowa', 'KS':'Kansas', 'KY':'Kentucky',\n",
    "        'LA':'Louisiana', 'ME':'Maine', 'MD':'Maryland', 'MA':'Massachusetts', 'MI':'Michigan',\n",
    "        'MN':'Minnesota', 'MS':'Mississippi', 'MO':'Missouri', 'MT':'Montana', 'NE':'Nebraska',\n",
    "        'NV':'Nevada', 'NH':'New Hampshire', 'NJ':'New Jersey', 'NM':'New Mexico', 'NY':'New York',\n",
    "        'NC':'North Carolina', 'ND':'North Dakota', 'MP':'Northern Mariana Islands', 'OH':'Ohio', 'OK':'Oklahoma',\n",
    "        'OR':'Oregon', 'PA':'Pennsylvania', 'PR':'Puerto Rico', 'RI':'Rhode Island', 'SC':'South Carolina',\n",
    "        'SD':'South Dakota', 'TN':'Tennessee', 'TX':'Texas', 'UT':'Utah', 'VT':'Vermont',\n",
    "        'VA':'Virginia', 'VI':'Virgin Islands', 'WA':'Washington', 'WV':'West Virginia', 'WI':'Wisconsin',\n",
    "        'WT':'Wyoming' }\n",
    "\n",
    "\n",
    "for i in dick:\n",
    "    location = geolocator.geocode(dick[i])\n",
    "    dick[i] = str((location.latitude, location.longitude))\n",
    "\n",
    "def convert_state(s):\n",
    "    return dick.get(s, s)\n",
    "\n",
    "conv_state = UserDefinedFunction(lambda s : convert_state(s), StringType())\n",
    "spark.udf.register(\"convert_state\", conv_state)\n",
    "\n",
    "explore = explore.withColumn('e', conv_state('CAND_OFFICE_ST'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36a20340-680a-452c-979a-e685034220ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.createOrReplaceTempView(\"explore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b1c68c8-fbbc-44b8-b1a9-5e0b2651cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.coalesce(1).write.format('json').save('dorathesisepuede')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85e17439-9cb9-408d-b51b-d2227afa0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtertbl.coalesce(1).write.format('json').save('explorefiltered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb04e90-e088-46bd-a7dd-25edb40cf672",
   "metadata": {},
   "outputs": [],
   "source": [
    "senate = spark.sql(\"\"\" SELECT * FROM explore WHERE CAND_OFFICE = \"S\" \"\"\")\n",
    "\n",
    "senate = senate.na.fill(value=0,subset=[\"CAND_OFFICE_DISTRICT\"])\n",
    "senate = senate.na.fill(value=\"-\",subset=[\"CAND_PCC\"])\n",
    "senate = senate.na.fill(value=\"-\",subset=[\"CAND_PTY_AFFILIATION\"])\n",
    "senate.createOrReplaceTempView(\"senate\")\n",
    "\n",
    "senate = spark.sql(\"\"\"\n",
    "SELECT (CAND_ELECTION_YR || CAND_OFFICE || CAND_OFFICE_ST || CAND_OFFICE_DISTRICT) AS CID, CAND_ID, CAND_NAME,CAND_PTY_AFFILIATION, CAND_ICI, CAND_ELECTION_YR, CAND_OFFICE_ST, CAND_OFFICE, CAND_OFFICE_DISTRICT, CAND_PCC\n",
    "FROM senate\n",
    "GROUP BY CID, CAND_ID, CAND_NAME, CAND_PTY_AFFILIATION, CAND_ICI, CAND_ELECTION_YR, CAND_OFFICE_ST, CAND_OFFICE, CAND_OFFICE_DISTRICT, CAND_PCC\n",
    "\n",
    "\"\"\")\n",
    "senate.createOrReplaceTempView(\"senate\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f401bf-4b99-4c8d-bbd8-6fee4ff9873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "house = spark.sql(\"\"\" SELECT * FROM explore WHERE CAND_OFFICE = \"H\" \"\"\")\n",
    "\n",
    "house = house.na.fill(value=0,subset=[\"CAND_OFFICE_DISTRICT\"])\n",
    "house = house.na.fill(value=\"-\",subset=[\"CAND_PCC\"])\n",
    "house = house.na.fill(value=\"-\",subset=[\"CAND_PTY_AFFILIATION\"])\n",
    "house.createOrReplaceTempView(\"house\")\n",
    "\n",
    "house = spark.sql(\"\"\"\n",
    "SELECT (CAND_ELECTION_YR || CAND_OFFICE || CAND_OFFICE_ST || CAND_OFFICE_DISTRICT) AS CID, CAND_ID, CAND_NAME,CAND_PTY_AFFILIATION, CAND_ICI, CAND_ELECTION_YR, CAND_OFFICE_ST, CAND_OFFICE, CAND_OFFICE_DISTRICT, CAND_PCC\n",
    "FROM house\n",
    "GROUP BY CID, CAND_ID, CAND_NAME, CAND_PTY_AFFILIATION, CAND_ICI, CAND_ELECTION_YR, CAND_OFFICE_ST, CAND_OFFICE, CAND_OFFICE_DISTRICT, CAND_PCC\n",
    "\n",
    "\"\"\")\n",
    "house.createOrReplaceTempView(\"house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58cf73a-8d70-4358-bfdb-1a78fc25bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pres = spark.sql(\"\"\" SELECT * FROM explore WHERE CAND_OFFICE = \"P\" \"\"\")\n",
    "\n",
    "pres = pres.na.fill(value=0,subset=[\"CAND_OFFICE_DISTRICT\"])\n",
    "pres = pres.na.fill(value=\"-\",subset=[\"CAND_PCC\"])\n",
    "pres = pres.na.fill(value=\"-\",subset=[\"CAND_PTY_AFFILIATION\"])\n",
    "pres.createOrReplaceTempView(\"pres\")\n",
    "\n",
    "pres = spark.sql(\"\"\"\n",
    "SELECT (CAND_ELECTION_YR || CAND_OFFICE || CAND_OFFICE_ST || CAND_OFFICE_DISTRICT) AS CID, CAND_ID, CAND_NAME,CAND_PTY_AFFILIATION, CAND_ICI, CAND_ELECTION_YR, CAND_OFFICE_ST, CAND_OFFICE, CAND_OFFICE_DISTRICT, CAND_PCC\n",
    "FROM pres\n",
    "GROUP BY CID, CAND_ID, CAND_NAME, CAND_PTY_AFFILIATION, CAND_ICI, CAND_ELECTION_YR, CAND_OFFICE_ST, CAND_OFFICE, CAND_OFFICE_DISTRICT, CAND_PCC\n",
    "\n",
    "\"\"\")\n",
    "pres.createOrReplaceTempView(\"pres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d7cc7-86dd-4058-aa72-dc0e96c1b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = senate.unionAll(pres)\n",
    "companal = n.unionAll(house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b735a8d-f749-46e5-9415-5c191a18e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "companal.createOrReplaceTempView(\"companal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1c941-a9fb-4637-9578-2283951262eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from geopy.geocoders import GoogleV3\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, collect_list, concat, lit, regexp_replace, round, split, struct, udf, UserDefinedFunction\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, ArrayType, StructType, StructField\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('analytics-tables').getOrCreate()\n",
    "\n",
    "allIndivSchema = StructType([\n",
    "    StructField(\"CMTE_ID\", StringType(), False),\n",
    "    StructField(\"AMNDT_IND\", StringType(), True),\n",
    "    StructField(\"RPT_TP\", StringType(), True),\n",
    "    StructField(\"TRANSACTION_PGI\", StringType(), True),\n",
    "    StructField(\"IMAGE_NUM\", StringType(), True),\n",
    "    StructField(\"TRANSACTION_TP\", StringType(), True),\n",
    "    StructField(\"ENTITY_TP\", StringType(), True),\n",
    "    StructField(\"NAME\", StringType(), True),\n",
    "    StructField(\"CITY\", StringType(), True),\n",
    "    StructField(\"STATE\", StringType(), True),\n",
    "    StructField(\"ZIP_CODE\", StringType(), True),\n",
    "    StructField(\"EMPLOYER\", StringType(), True),\n",
    "    StructField(\"OCCUPATION\", StringType(), True),\n",
    "    StructField(\"TRANSACTION_DT\", IntegerType(), True),\n",
    "    StructField(\"TRANSACTION_AMT\", IntegerType(), True),\n",
    "    StructField(\"OTHER_ID\", StringType(), True),\n",
    "    StructField(\"TRAN_ID\", StringType(), True),\n",
    "    StructField(\"FILE_NUM\", IntegerType(), True),\n",
    "    StructField(\"MEMO_CD\", StringType(), True),\n",
    "    StructField(\"MEMO_TEXT\", StringType(), True),\n",
    "    StructField(\"SUB_ID\", StringType(), True)\n",
    "])\n",
    "\n",
    "allIndiv = spark.read.format(\"csv\").options(delimiter='|',header='true').schema(allIndivSchema).load(\"data/indiv_cont/itcon*.txt\")\n",
    "allIndiv = allIndiv.drop(\"AMNDT_IND\",\"RPT_TP\",\"TRANSACTION_PGI\",\"IMAGE_NUM\",\"TRANSACTION_TP\",\"NAME\",\"CITY\",\"ZIP_CODE\",\"EMPLOYER\",\"OCCUPATION\",\"TRANSACTION_DT\",\"TRANSACTION_TP\",\"OTHER_ID\",\"TRAN_ID\",\"FILE_NUM\",\"MEMO_CD\",\"MEMO_TEXT\",\"SUB_ID\")\n",
    "allIndiv.createOrReplaceTempView(\"allInd\")\n",
    "\n",
    "commToCand = spark.read.format(\"csv\").options(delimiter='|',inferschema='true',header='true').load('data/itpas2.txt')\n",
    "commToCand = commToCand.drop('CMTE_ID',\"AMNDT_IND\",\"RPT_TP\",\"TRANSACTION_PGI\",\"IMAGE_NUM\",\"TRANSACTION_TP\",\"ENTITY_TP\",\"NAME\",\"CITY\",\"ZIP_CODE\",\"EMPLOYER\",\"OCCUPATION\",\"TRANSACTION_DT\",\"TRANSACTION_TP\",\"CAND_ID\",\"TRAN_ID\",\"FILE_NUM\",\"MEMO_CD\",\"MEMO_TEXT\",\"SUB_ID\")\n",
    "commToCand.createOrReplaceTempView(\"allCom\")\n",
    "\n",
    "candMaster = spark.read.format(\"csv\").options(delimiter='|',inferschema='true',header='true').load('data/cn.txt')\n",
    "candMaster = candMaster.drop(\"CAND_NAME\",\"CAND_PTY_AFFILIATION\",\"CAND_ELECTION_YR\",\"CAND_OFFICE_ST\",\"CAND_OFFICE\",\"CAND_OFFICE_DISTRICT\",\"CAND_ICI\",\"CAND_STATUS\",\"CAND_ST1\",\"CAND_ST2\",\"CAND_CITY\",\"CAND_ST\",\"CAND_ZIP\")\n",
    "candMaster.createOrReplaceTempView(\"candMast\")\n",
    "\n",
    "comMaster = spark.read.format(\"csv\").options(delimiter='|',inferschema='true',header='true').load('data/cm.txt')\n",
    "comMaster = comMaster.drop(\"CMTE_NM\",\"TRES_NM\",\"CMTE_ST1\",\"CMTE_ST2\",\"CMTE_CITY\",\"CMTE_ST\",\"CMTE_ZIP\",\"CMTE_DSGN\",\"CMTE_TP\",\"CMTE_PTY_AFFILIATION\",\"CMTE_FILING_FREQ\",\"ORG_TP\",\"CONNECTED_ORG_NM\")\n",
    "comMaster.createOrReplaceTempView(\"comMast\")\n",
    "\n",
    "print(allIndiv.dtypes)\n",
    "print(commToCand.dtypes)\n",
    "print(candMaster.dtypes)\n",
    "print(comMaster.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585933e2-8522-43cd-bc9c-7bf0c4cf382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = GoogleV3(api_key=\"AIzaSyC0GaU8jaB9fesUuG7sAQz07g-R5ZRBP7Q\")\n",
    "\n",
    "dick = { 'AL':'Alabama', 'AK':'Alaska', 'AR':'Arizona', 'AR':'Arkansas', 'AS':'American Samoa',\n",
    "        'CA':'California', 'CO':'Colorado', 'CT':'Connecticut', 'DE':'Delaware', 'DC':'District of Columbia',\n",
    "        'FL':'Florida', 'GA':'Georgia', 'GU':'Guam', 'HI':'Hawaii', 'ID':'Idaho',\n",
    "        'IL':'Illinois', 'IN':'Indiana', 'IA':'Iowa', 'KS':'Kansas', 'KY':'Kentucky',\n",
    "        'LA':'Louisiana', 'ME':'Maine', 'MD':'Maryland', 'MA':'Massachusetts', 'MI':'Michigan',\n",
    "        'MN':'Minnesota', 'MS':'Mississippi', 'MO':'Missouri', 'MT':'Montana', 'NE':'Nebraska',\n",
    "        'NV':'Nevada', 'NH':'New Hampshire', 'NJ':'New Jersey', 'NM':'New Mexico', 'NY':'New York',\n",
    "        'NC':'North Carolina', 'ND':'North Dakota', 'MP':'Northern Mariana Islands', 'OH':'Ohio', 'OK':'Oklahoma',\n",
    "        'OR':'Oregon', 'PA':'Pennsylvania', 'PR':'Puerto Rico', 'RI':'Rhode Island', 'SC':'South Carolina',\n",
    "        'SD':'South Dakota', 'TN':'Tennessee', 'TX':'Texas', 'UT':'Utah', 'VT':'Vermont',\n",
    "        'VA':'Virginia', 'VI':'Virgin Islands', 'WA':'Washington', 'WV':'West Virginia', 'WI':'Wisconsin',\n",
    "        'WT':'Wyoming' }\n",
    "\n",
    "for i in dick:\n",
    "    location = geolocator.geocode(dick[i])\n",
    "    dick[i] = str((location.latitude, location.longitude))\n",
    "\n",
    "def convert_state(s):\n",
    "    return dick.get(s, s)\n",
    "\n",
    "conv_state = UserDefinedFunction(lambda s : convert_state(s), StringType())\n",
    "spark.udf.register(\"convert_state\", conv_state)\n",
    "\n",
    "stateTableALL = spark.sql(\"\"\"\n",
    "SELECT stateIND.CAND_ID AS ID, stateIND.STATE,\n",
    "    convert_state(stateIND.STATE) AS COORD,\n",
    "    (CASE WHEN stateIND.TOTAL_AMT_IND IS NULL THEN 0 ELSE stateIND.TOTAL_AMT_IND END)+\n",
    "    (CASE WHEN stateCOM.TOTAL_AMT_COM IS NULL THEN 0 ELSE stateCOM.TOTAL_AMT_COM END) AS TOTAL,\n",
    "    (CASE WHEN stateIND.TOTAL_AMT_IND IS NULL THEN 0 ELSE stateIND.TOTAL_AMT_IND END) AS INDIVIDUAL,\n",
    "    (CASE WHEN stateCOM.TOTAL_AMT_COM IS NULL THEN 0 ELSE stateCOM.TOTAL_AMT_COM END) AS COMMITTEE\n",
    "FROM (    \n",
    "    SELECT candMast.CAND_ID,\n",
    "        (CASE WHEN allInd.STATE IS NULL THEN \"--\" ELSE allInd.STATE END) AS STATE,\n",
    "        SUM(allInd.TRANSACTION_AMT) AS TOTAL_AMT_IND\n",
    "    FROM allInd, candMast, comMast\n",
    "    WHERE comMast.CAND_ID=candMast.CAND_ID AND (comMast.CMTE_ID=allInd.CMTE_ID OR comMast.CAND_ID=allInd.CMTE_ID) AND (allInd.ENTITY_TP=\"CAN\" OR allInd.ENTITY_TP=\"IND\")\n",
    "    GROUP BY candMast.CAND_ID, STATE\n",
    "    ) AS stateIND\n",
    "FULL JOIN (\n",
    "    SELECT candMast.CAND_ID,\n",
    "        (CASE WHEN allCom.STATE IS NULL THEN \"--\" ELSE allCom.STATE END) AS STATE,\n",
    "        SUM(allCom.TRANSACTION_AMT) AS TOTAL_AMT_COM\n",
    "    FROM allCom, candMast, comMast\n",
    "    WHERE comMast.CAND_ID=candMast.CAND_ID AND (comMast.CMTE_ID=allCom.OTHER_ID OR comMast.CAND_ID=allCom.OTHER_ID)\n",
    "    GROUP BY candMast.CAND_ID, STATE\n",
    "    ) AS stateCOM\n",
    "WHERE stateIND.CAND_ID=stateCOM.CAND_ID AND stateIND.STATE=stateCOM.STATE\n",
    "ORDER BY ID, TOTAL DESC\n",
    "\"\"\")\n",
    "stateTableALL.createOrReplaceTempView(\"stateTablePre\")\n",
    "type(stateTableALL)\n",
    "stateTableALL.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c130632-d860-400f-830a-4ef1788bc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtable = stateTableALL.drop(\"STATE\", \"COORD\", \"TOTAL\")\n",
    "dtable.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e7a9018-d76a-4358-8240-a5965a73b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtable.createOrReplaceTempView(\"dtable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33335f38-8c50-468a-a0c0-9e1b87d2a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = spark.sql(\"\"\" SELECT * FROM companal a, dtable d WHERE a.CAND_ID = d.ID \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fee8420-38ea-42dc-a86e-e747697ac14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = combined.drop(\"ID\", \"CAND_ELECTION_YR\", \"CAND_OFFICE_ST\", \"CAND_OFFICE\", \"CAND_OFFICE_DISTRICT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e9fb5-bbaf-42cb-9268-023c700ba8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.printSchema()\n",
    "combined.createOrReplaceTempView(\"combined\")\n",
    "combined = spark.sql(\"\"\" SELECT CID, CAND_ID, CAND_NAME, CAND_PTY_AFFILIATION, CAND_PCC, CAND_ICI, SUM(INDIVIDUAL) AS INDIV, SUM(COMMITTEE) AS COMMIE FROM combined GROUP BY CID, CAND_ID, CAND_NAME, CAND_PTY_AFFILIATION, CAND_ICI, CAND_PCC  \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8575c-c06a-48df-9606-4b093a33e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.createOrReplaceTempView(\"combined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a2490-6580-4f54-9f19-330671b8642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueCID = spark.sql(\"\"\"\n",
    "SELECT DISTINCT CID\n",
    "FROM combined\n",
    "\"\"\")\n",
    "\n",
    "ftable = spark.sql(\"\"\"\n",
    "SELECT CID, CAND_ID, CAND_NAME, CAND_PTY_AFFILIATION, INDIV, COMMIE, CAND_PCC, CAND_ICI\n",
    "FROM combined\n",
    "\"\"\")\n",
    "\n",
    "# With an outer equi-join by (ID, NAME), we nest one or more rows of (STATE, TOTAL, INDIVIDUAL, COMMITTEE) columns for each distinct candidate using the struct type\n",
    "finalTable = uniqueCID.join(\n",
    "    ftable\n",
    "        .groupBy(\"CID\")\n",
    "        .agg(collect_list(struct(ftable.CAND_ID, ftable.CAND_NAME, ftable.CAND_PTY_AFFILIATION, ftable.INDIV, ftable.COMMIE, ftable.CAND_PCC, ftable.CAND_ICI)).alias(\"competitors\"))\n",
    "    , \"CID\"\n",
    "    , \"outer\"\n",
    ")\n",
    "\n",
    "finalTable.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd00d7-cd99-442b-9c49-f774e4e0f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTable.coalesce(1).write.format('json').save('anal_competition')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
